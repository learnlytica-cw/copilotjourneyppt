= GitGub CoPilot Fundamentals
Author Name
:icons: font
:revealjs_theme: white
:revealjs_slideNumber: c/t

// Optional slide options (use this if you're generating slides with Asciidoctor Reveal.js)
:revealjs_transition: slide
:revealjs_width: 1280
:revealjs_height: 720

== Slide 1: Generative AI amd LLMs
[.text-center]
== Welcome to the Presentation
This presentation covers the following topics:

* Power of Generative AI​
* Technical Foundation of Generative AI​
* Opportunities Created by Generative AI​
* Challenges and Key Concerns​
* Generative AI and LLM​
* Common Generative AI Applications​
* ChatGPT vs GitHub Copilot​



== Slide 2: Power of Generative AI
=== Overview 
[.text-left]

* Generative AI is one of the most powerful advances in technology ever.​
* It enables developers to build applications that consume machine learning models trained with a large volume of data from across the Internet to generate new content that can be indistinguishable from content created by a human.​
* With such powerful capabilities, generative AI brings with it some dangers; and requires that data scientists, developers, and others involved in creating generative AI solutions adopt a responsible approach that identifies, measures, and mitigates risks.​



== Slide 3: Technical Foundation of GenAI

[.text-left]

* Model Architecture
* Self Supervised Pre_Training
* Generative Modeling Methods

NOTE: Model architecture: The structure and design of the neural networks that generate the content. Examples of model architectures include transformers, convolutional neural networks, recurrent neural networks, and attention mechanisms.​

NOTE: Self-supervised pretraining: The process of training a model on a large amount of unlabeled data is to learn general features and patterns that can be transferred to specific tasks. Examples of self-supervised pretraining methods include masked language modeling, contrastive learning, and denoising autoencoders.​

NOTE: Generative modeling methods: The techniques and algorithms that enable a model to learn the probability distribution of the data and sample new data from it.​

== Slide 4: Opportunities Created by GenAI

[.text-left]

* Enhancing Creativity and Innovation
* Improving efficiency and productivity
* Increasing accessibility and diversity

NOTE: Enhancing creativity and innovation: Generative AI can augment human creativity by providing new ideas, insights, and perspectives that can inspire novel solutions and products.​

NOTE: Improving efficiency and productivity: Generative AI can automate tedious and repetitive tasks that are time intensive and require significant resources, such as data collection, data labeling, data analysis, and content creation.​

NOTE: Increasing accessibility and diversity: Generative AI can make content and services more accessible and inclusive by generating personalized and customized content that meets the needs and preferences of different users and audiences.​

== Slide 5: Challenges of GenAI

[.text-left]

* Ensuring quality and reliability
* Maintaining security and privacy
* Regulating ethics and responsibility

NOTE: Ensuring quality and reliability: Generative AI can produce inaccurate or misleading content that can harm the reputation or performance of a business or an Examples of quality and reliability issues include factual errors, logical inconsistencies, grammatical mistakes, and plagiarism.​

NOTE: Maintaining security and privacy: Generative AI can pose risks to the security and privacy of data and systems by enabling malicious actors to generate fake or spoofed content that can deceive or manipulate users or bypass authentication mechanisms. Examples of security and privacy issues include identity theft, fraud, phishing, deepfakes, cyberattacks, and data breaches.​

NOTE:Regulating ethics and responsibility: Generative AI can raise ethical and social questions about the ownership, accountability, transparency, explainable, fairness, trustworthiness, and impact of the generated content on individuals and society.​

== Slide 5: Key Concerns of GenAI

image::use_cases_img.png[Topic Image, width=50%]

[.text-left]

* High energy usage: An analysis has shown that training an LLM model with 200 billion parameters produces approximately 75,000 kg of CO2 emissions​
* Model bias: Presence of systematic and unfair inaccuracies or prejudices in the predictions or decisions made by a machine learning model.​
* Toxic comments: A toxic comment is a text-based input, usually in the form of a comment, message, or text snippet, that contains harmful, offensive, or inappropriate content.​
* Hallucination: Hallucination refers to Generative AI responses that are produced when the search context changes, or a response is not supported by the underlying data. Hallucination results in query responses that are illogical and deceptive​
* Conversational AI leakage: Conversational AI leakage occurs when sensitive data is input into a LLM and is unintentionally exposed​

== Generative AI and LLMs

[.text-left]

* Generative AI, large language models and foundation models are similar, but different and are commonly used interchangeably. ​
* There is not a clear demarcation between terms, and this becomes challenging when a needed delineation is required. ​
* Generative AI is a broad term that can be used for any AI system whose primary function is to generate content. ​
* Large language models (LLMs) are a type of AI system that works with language. The LLM aims to model language, i.e., to create a simplified—but useful—digital representation.​
* Both Generative AI and LLM models extract value from enormous data sets and provide straightforward learning in an accessible manne​


  == Pretrained LLMS

  [.text-left]

* ChatGPT, by OpenAI, can generate an answer to almost any question it’s asked and is free to use. ChatGPT using the GPT-4 LLM, released in 2023, is the latest version as of this writing and is unique in that it is multimodal, which can process a combination of image and text inputs to produce text outputs.​
* DALL-E2, developed by OpenAI, creates AI-generated images that are realistic and art from descriptive natural language. DALL-E2 can merge styles and concepts in new creative ways.​
* LLAMA 2, by Meta, is open-source LLM series based on up to 70 billion parameters and trained on 2 trillion tokens. Smaller versions exist that can be fine-tuning for a variety of tasks. This model is open source and available for research and commercial purposes.​
* BLOOM, by BigScience, is an open-source multilingual large language model, which can generate text in 46 languages and 13 programming languages. Soon, an inference API will be released.​
* LaMDA 2, language model for dialogue applications by Google, is an advanced AI-driven chatbot with conversational skills trained on dialogue and is built on a transformer neural network architecture.​
* MPT-7B-Instruct is built on a modified decoder-only transformer architecture and has ~ 7 billion parameters trained on 1 trillion tokens from a variety of text datasets. It is open-source, which can be used commercially and is a model for short-form instruction following.​

== LLMs Use-Cases

  
== Slide 5: Summary
=== Key Takeaways
* Key takeaway 1
* Key takeaway 2
* Key takeaway 3

== Slide 6: Q & A
=== Questions?
[.text-center]
Thank you! Any questions?
